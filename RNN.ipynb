{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_forward(x, prev_h, Wx, Wh, b):\n",
    "    \"\"\"\n",
    "    Run the forward pass for a single timestep of a vanilla RNN that uses a tanh\n",
    "    activation function.\n",
    "\n",
    "    The input data has dimension D, the hidden state has dimension H, and we use\n",
    "    a minibatch size of N.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data for this timestep, of shape (N, D).\n",
    "    - prev_h: Hidden state from previous timestep, of shape (N, H)\n",
    "    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)\n",
    "    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)\n",
    "    - b: Biases of shape (H,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - next_h: Next hidden state, of shape (N, H)\n",
    "    - cache: Tuple of values needed for the backward pass.\n",
    "    \"\"\"\n",
    "    \n",
    "    next_h = np.tanh(np.dot(x, Wx) + np.dot(prev_h, Wh) + b)\n",
    "    cache = (Wx, Wh, b, x, prev_h, next_h)\n",
    "    \n",
    "    return next_h, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_backward(dnext_h, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for a single timestep of a vanilla RNN.\n",
    "\n",
    "    Inputs:\n",
    "    - dnext_h: Gradient of loss with respect to next hidden state, of shape (N, H)\n",
    "    - cache: Cache object from the forward pass\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradients of input data, of shape (N, D)\n",
    "    - dprev_h: Gradients of previous hidden state, of shape (N, H)\n",
    "    - dWx: Gradients of input-to-hidden weights, of shape (D, H)\n",
    "    - dWh: Gradients of hidden-to-hidden weights, of shape (H, H)\n",
    "    - db: Gradients of bias vector, of shape (H,)\n",
    "    \"\"\"\n",
    "    Wx, Wh, b, x, prev_h, next_h = cache\n",
    "    \n",
    "    dz = dnext_h * (1 - next_h ** 2)\n",
    "    dx = np.dot(dz, Wx.T)\n",
    "    dprev_h = np.dot(dz, Wh.T)\n",
    "    dWx = np.dot(x.T, dz)\n",
    "    dWh = np.dot(prev_h.T, dz)\n",
    "    db = np.sum(dz, axis=0)\n",
    "    \n",
    "    return dx, dprev_h, dWx, dWh, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(x, h0, Wx, Wh, b):\n",
    "    \"\"\"\n",
    "    Run a vanilla RNN forward on an entire sequence of data. We assume an input\n",
    "    sequence composed of T vectors, each of dimension D. The RNN uses a hidden\n",
    "    size of H, and we work over a minibatch containing N sequences. After running\n",
    "    the RNN forward, we return the hidden states for all timesteps.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data for the entire timeseries, of shape (N, T, D).\n",
    "    - h0: Initial hidden state, of shape (N, H)\n",
    "    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)\n",
    "    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)\n",
    "    - b: Biases of shape (H,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - h: Hidden states for the entire timeseries, of shape (N, T, H).\n",
    "    - cache: Values needed in the backward pass\n",
    "    \"\"\"\n",
    "    N, T, D = x.shape\n",
    "    H, = b.shape\n",
    "    \n",
    "    cache = []\n",
    "    prev_h = h0\n",
    "    h = np.zeros((N, T, H))\n",
    "    \n",
    "    for i in range(T):\n",
    "        prev_h, tmp_cache = rnn_step_forward(x[:,i,:], prev_h, Wx, Wh, b)\n",
    "        h[:,i,:] = prev_h\n",
    "        cache.append(tmp_cache)\n",
    "    \n",
    "    return h, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_backward(dh, cache):\n",
    "    \"\"\"\n",
    "    Compute the backward pass for a vanilla RNN over an entire sequence of data.\n",
    "\n",
    "    Inputs:\n",
    "    - dh: Upstream gradients of all hidden states, of shape (N, T, H). \n",
    "    \n",
    "    NOTE: 'dh' contains the upstream gradients produced by the \n",
    "    individual loss functions at each timestep, *not* the gradients\n",
    "    being passed between timesteps (which you'll have to compute yourself\n",
    "    by calling rnn_step_backward in a loop).\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient of inputs, of shape (N, T, D)\n",
    "    - dh0: Gradient of initial hidden state, of shape (N, H)\n",
    "    - dWx: Gradient of input-to-hidden weights, of shape (D, H)\n",
    "    - dWh: Gradient of hidden-to-hidden weights, of shape (H, H)\n",
    "    - db: Gradient of biases, of shape (H,)\n",
    "    \"\"\"\n",
    "    \n",
    "    N, T, H = dh.shape\n",
    "    D = cache[0][0].shape[0]\n",
    "    \n",
    "    dprev_h = np.zeros((N,H))\n",
    "    dx = np.zeros((N, T, D))\n",
    "    dWx = np.zeros((D,H))\n",
    "    dWh = np.zeros((H,H))\n",
    "    db = np.zeros(H)\n",
    "    \n",
    "    for i in reversed(range(T)):\n",
    "        dcurr_h = dprev_h + dh[:,i,:]\n",
    "        dx[:,i,:], dprev_h, tmp_dWx, tmp_dWh, tmp_db = rnn_step_backward(dcurr_h, cache[i])\n",
    "        dWx += tmp_dWx\n",
    "        dWh += tmp_dWh\n",
    "        db += tmp_db\n",
    "    \n",
    "    dh0 = dprev_h\n",
    "    \n",
    "    return dx, dh0, dWx, dWh, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
