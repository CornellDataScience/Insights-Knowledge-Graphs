{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformer.flow import make_model, batch_size_fn, run_epoch\n",
    "\n",
    "vocab_src = torch.load(\"./noie_full_6heads_2048ffsrc_vocab.pt\")\n",
    "vocab_tgt = torch.load(\"./noie_full_6heads_2048fftrg_vocab.pt\")\n",
    "model = make_model(len(vocab_src), len(vocab_tgt),\n",
    "                        n=5, d_model=768,\n",
    "                        d_ff=1200, h=6,\n",
    "                        dropout=.1)\n",
    "device = torch.device('cpu')\n",
    "model.load_state_dict(torch.load('./noie_full_1200ff_6heads_r10_epoch0.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (3): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (feed_forward): PointerwiseFeedforward(\n",
       "          (w_1): Linear(in_features=768, out_features=1200, bias=True)\n",
       "          (w_2): Linear(in_features=1200, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (3): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (feed_forward): PointerwiseFeedforward(\n",
       "          (w_1): Linear(in_features=768, out_features=1200, bias=True)\n",
       "          (w_2): Linear(in_features=1200, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (3): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (feed_forward): PointerwiseFeedforward(\n",
       "          (w_1): Linear(in_features=768, out_features=1200, bias=True)\n",
       "          (w_2): Linear(in_features=1200, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): EncoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (3): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (feed_forward): PointerwiseFeedforward(\n",
       "          (w_1): Linear(in_features=768, out_features=1200, bias=True)\n",
       "          (w_2): Linear(in_features=1200, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): EncoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (3): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (feed_forward): PointerwiseFeedforward(\n",
       "          (w_1): Linear(in_features=768, out_features=1200, bias=True)\n",
       "          (w_2): Linear(in_features=1200, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (3): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (src_attn): MultiHeadAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (3): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (feed_forward): PointerwiseFeedforward(\n",
       "          (w_1): Linear(in_features=768, out_features=1200, bias=True)\n",
       "          (w_2): Linear(in_features=1200, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (3): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (src_attn): MultiHeadAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (3): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (feed_forward): PointerwiseFeedforward(\n",
       "          (w_1): Linear(in_features=768, out_features=1200, bias=True)\n",
       "          (w_2): Linear(in_features=1200, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (3): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (src_attn): MultiHeadAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (3): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (feed_forward): PointerwiseFeedforward(\n",
       "          (w_1): Linear(in_features=768, out_features=1200, bias=True)\n",
       "          (w_2): Linear(in_features=1200, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (3): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (src_attn): MultiHeadAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (3): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (feed_forward): PointerwiseFeedforward(\n",
       "          (w_1): Linear(in_features=768, out_features=1200, bias=True)\n",
       "          (w_2): Linear(in_features=1200, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (3): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (src_attn): MultiHeadAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (3): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (feed_forward): PointerwiseFeedforward(\n",
       "          (w_1): Linear(in_features=768, out_features=1200, bias=True)\n",
       "          (w_2): Linear(in_features=1200, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "  )\n",
       "  (src_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(32216, 768)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (tgt_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(29171, 768)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (proj): Linear(in_features=768, out_features=29171, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import OpenAIGPTTokenizer, OpenAIGPTModel\n",
    "\n",
    "\n",
    "def get_arg_tokenizer():\n",
    "    tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
    "    max_key = max(tokenizer.encoder.values())\n",
    "    add_toks = ['argonestart', 'argoneend', 'relstart', 'relend', 'argtwostart', 'argtwoend']\n",
    "    word_piece = '</w>'\n",
    "    for ix, tok in enumerate(add_toks):\n",
    "        tokenizer.encoder[tok + word_piece] = max_key + 1 + ix\n",
    "        tokenizer.decoder[max_key + 1 + ix] = tok + word_piece\n",
    "        tokenizer.cache[tok] = tok + word_piece\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('./all.txt', encoding='UTF-8') as f:\n",
    "#         test_data = f.read()\n",
    "# a = test_data.split('\\n')\n",
    "# a    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok = get_arg_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformer.greedy import greedy_decode\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1038,     3,   477,     5,  6930,  1411,  1931,    10,  4203,    16,\n",
      "         10199, 13278,  5011,    88,  6939, 11797,  9571,    17,  8944,     4]])\n"
     ]
    }
   ],
   "source": [
    "sent = tok.tokenize('Instead , much of numerical analysis is concerned with obtaining approximate solutions while maintaining reasonable bounds on errors .')\n",
    "src = torch.LongTensor([[vocab_src.stoi[w] for w in sent]])\n",
    "print(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]],\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = Variable(src)\n",
    "src_mask = (src != vocab_src.stoi[\"<blank>\"]).unsqueeze(-2)\n",
    "src_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Argument and relation boundaries:\t<s> argonestart</w> much</w> of</w> numer ical</w> analysis</w> argoneend</w> relstart</w> is</w> relend</w> argtwostart</w> concerned</w> with</w> obtaining</w> approximate</w> solutions</w> argtwoend</w> \n"
     ]
    }
   ],
   "source": [
    "out = greedy_decode(model, src, src_mask, \n",
    "                    max_len=60, start_symbol=vocab_tgt.stoi[\"<s>\"])\n",
    "print(\"Argument and relation boundaries:\", end=\"\\t\")\n",
    "trans = \"<s> \"\n",
    "for i in range(1, out.size(1)):\n",
    "    sym = vocab_tgt.itos[out[0, i]]\n",
    "    if sym == \"</s>\": break\n",
    "    trans += sym + \" \"\n",
    "print(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
